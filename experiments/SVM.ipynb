{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4757998-23ae-4e79-bdf2-306a98f38171",
   "metadata": {},
   "source": [
    "# Here we just steal tests from Sam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cf1b40-7ad6-4834-9bb4-d21af63277b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0. Imports and misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4748f-239b-49ad-8ee9-88d542e07397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools as it; import more_itertools as mit\n",
    "import functools as ft\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from numpy.random import default_rng; rng=default_rng()\n",
    "import seaborn as sns\n",
    "from sklearn.utils._testing import ignore_warnings #green flag: good code ahead\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import torch.utils.data\n",
    "\n",
    "from ot_markov_distances import discounted_wl_infty\n",
    "from ot_markov_distances.utils import weighted_transition_matrix\n",
    "from ml_lib.misc import auto_repr, all_equal\n",
    "\n",
    "\n",
    "def append_path(s):\n",
    "    s = os.path.abspath(s)\n",
    "    if s in sys.path: return\n",
    "    sys.path.insert(0,s)\n",
    "append_path(\"\")\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import numpy as np\n",
    "#from svm import grakel_to_nx, grakel_to_igraph\n",
    "from grakel import GraphKernel, Graph\n",
    "import itertools\n",
    "#from grakel.datasets import fetch_dataset\n",
    "import grakel.datasets\n",
    "import networkx as nx\n",
    "import time\n",
    "import ot\n",
    "import sys\n",
    "import wwl\n",
    "import igraph as ig\n",
    "from tqdm import trange, tqdm\n",
    "import cProfile\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2a2916-8703-496b-ade5-ca4ababd76c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "OK just monkey-patch not working function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "286cb4ee-48e1-4b38-b38b-b572636ad5af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unittest.mock import patch\n",
    "from urllib.request import urlopen, HTTPError\n",
    "import ssl\n",
    "from shutil import copyfileobj\n",
    "\n",
    "#monkey patch this\n",
    "def _download_zip(url, output_name):\n",
    "    \"\"\"Download a file from a requested url and store locally.\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The url from where the file will be downloaded.\n",
    "    output_name : str\n",
    "        The name of the file in the local directory.\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    #ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n",
    "    context = ssl._create_unverified_context()\n",
    "    filename = output_name + \".zip\"\n",
    "    try:\n",
    "        data_url = urlopen(url, context=context)\n",
    "    except HTTPError as e:\n",
    "        if e.code == 404:\n",
    "            e.msg = \"Dataset '%s' not found on mldata.org.\" % output_name\n",
    "        raise\n",
    "\n",
    "    # Store Zip File\n",
    "    try:\n",
    "        with open(filename, 'w+b') as zip_file:\n",
    "            copyfileobj(data_url, zip_file)\n",
    "    except Exception:\n",
    "        os.remove(filename)\n",
    "        raise\n",
    "    data_url.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e0a308-dd20-457a-9c26-c754c09ec56b",
   "metadata": {},
   "source": [
    "And steal some stuff from Sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12ead1ed-5443-48df-bd5d-6a5a1f0bba97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grakel_to_nx(G, include_attr=True):\n",
    "    nx_G = []\n",
    "    for graph in G:\n",
    "        adj_mat = graph.get_adjacency_matrix()\n",
    "        nx_graph = nx.from_numpy_array(adj_mat)\n",
    "        nodes = sorted(list(graph.node_labels.keys()))\n",
    "        if include_attr == True:\n",
    "            for i in range(adj_mat.shape[0]):\n",
    "                nx_graph.nodes[i][\"attr\"] = graph.node_labels[nodes[i]]\n",
    "        nx_G.append(nx_graph)\n",
    "    return nx_G\n",
    "\n",
    "def grakel_to_igraph(G, add_attr=False):\n",
    "    lst = []\n",
    "    attr_list = []\n",
    "    max_nodes = 0\n",
    "    szs = []\n",
    "    for graph in G:\n",
    "        adj_mat = graph.get_adjacency_matrix()\n",
    "        igraph = ig.Graph.Adjacency(adj_mat)\n",
    "        n = adj_mat.shape[0]\n",
    "        if add_attr:\n",
    "            nodes = sorted(list(graph.node_labels.keys()))\n",
    "            attrs = []\n",
    "            for i in range(len(nodes)):\n",
    "                attrs.append(graph.node_labels[nodes[i]])\n",
    "            igraph.vs[\"label\"] = attrs\n",
    "        if n > max_nodes:\n",
    "            max_nodes = n\n",
    "        lst.append(igraph)\n",
    "    if add_attr:\n",
    "        for graph in G:\n",
    "            nodes = sorted(list(graph.node_labels.keys()))\n",
    "            attrs = [0]*max_nodes\n",
    "            for i in range(graph.n):\n",
    "                attrs[i] = graph.node_labels[nodes[i]]\n",
    "            attr_list.append(attrs)\n",
    "    return lst, attr_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ede023-fde3-4e5e-bf9c-7893d48d4c8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4aad380d-7f94-41ef-85a3-b89e455888ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_titles = [\"MUTAG\", \"PTC_MR\", \"PTC_FM\",  \"COX2\", \"PROTEINS\", \"PROTEINS_full\", \"ENZYMES\"]\n",
    "ds_names = [ \"mutag\", \"ptc_mr\", \"ptc_fm\",\"cox2\", \"proteins\", \"proteins_full\", \"ENZYMES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f9e104ef-f658-430b-a890-d66c806328a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset  MUTAG..\n",
      "Parsing dataset  MUTAG..\n",
      "Parse was succesful..\n",
      "Deleting unzipped dataset files..\n",
      "Going back to the original directory..\n",
      "Extracting dataset  PTC_MR..\n",
      "Parsing dataset  PTC_MR..\n",
      "Parse was succesful..\n",
      "Deleting unzipped dataset files..\n",
      "Going back to the original directory..\n",
      "Extracting dataset  PTC_FM..\n",
      "Parsing dataset  PTC_FM..\n",
      "Parse was succesful..\n",
      "Deleting unzipped dataset files..\n",
      "Going back to the original directory..\n",
      "Extracting dataset  COX2..\n",
      "Parsing dataset  COX2..\n",
      "Parse was succesful..\n",
      "Deleting unzipped dataset files..\n",
      "Going back to the original directory..\n",
      "Extracting dataset  PROTEINS..\n",
      "Parsing dataset  PROTEINS..\n",
      "Parse was succesful..\n",
      "Deleting unzipped dataset files..\n",
      "Going back to the original directory..\n",
      "Extracting dataset  PROTEINS_full..\n",
      "Parsing dataset  PROTEINS_full..\n",
      "Parse was succesful..\n",
      "Deleting unzipped dataset files..\n",
      "Going back to the original directory..\n",
      "Extracting dataset  ENZYMES..\n",
      "Parsing dataset  ENZYMES..\n",
      "Parse was succesful..\n",
      "Deleting unzipped dataset files..\n",
      "Going back to the original directory..\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "for dataset_name, dataset_title in zip(ds_names, dataset_titles):\n",
    "    with patch(\"grakel.datasets.base._download_zip\", _download_zip):    \n",
    "        ds = grakel.datasets.fetch_dataset(dataset_title, as_graphs=True)\n",
    "    G = ds.data\n",
    "    nx_G = grakel_to_nx(G, include_attr=True)\n",
    "    #igraphs, graph_attrs = grakel_to_igraph(G, add_attr=False)\n",
    "    y = ds.target\n",
    "    datasets.append((nx_G, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "063ffb41-8f02-4167-abcc-bc63e6639402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbc5485d-072a-46ea-b963-8de8e6572ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attr': 0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_index = -1\n",
    "data, gt = datasets[dataset_index]\n",
    "data[0].nodes[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693f8284-3f1c-4f16-af4a-795cec7eeeb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9f313374-1c47-4bc1-9459-a87c8d305aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17578/17578 [57:47<00:00,  5.07it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2851, 0.2843,  ..., 0.2583, 0.4739, 0.3491],\n",
       "        [0.2851, 0.0000, 0.1703,  ..., 0.1506, 0.3204, 0.2203],\n",
       "        [0.2843, 0.1703, 0.0000,  ..., 0.1498, 0.3196, 0.2196],\n",
       "        ...,\n",
       "        [0.2583, 0.1506, 0.1498,  ..., 0.0000, 0.2919, 0.1969],\n",
       "        [0.4739, 0.3204, 0.3196,  ..., 0.2919, 0.0000, 0.3884],\n",
       "        [0.3491, 0.2203, 0.2196,  ..., 0.1969, 0.3884, 0.0000]],\n",
       "       device='cuda:7')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_distance_matrix( set1, set2, wl_parameters, device=device, q=.1):\n",
    "    distance_matrix = torch.zeros(len(set1), len(set2), device=device)\n",
    "    \n",
    "    if set1 is set2:\n",
    "        #only compute the upper triangular part\n",
    "        iterator = it.combinations(enumerate(set1), 2)\n",
    "        iterator = (((clip1_i, clip1), (clip2_i, clip2))\n",
    "                    for (clip1_i, clip1), (clip2_i, clip2) in iterator\n",
    "                    if clip1_i != clip2_i\n",
    "                    )\n",
    "        n_iter = (len(set1) * (len(set1) - 1) // 2 )\n",
    "    else: \n",
    "        iterator = it.product(enumerate(set1), enumerate(set2))\n",
    "        n_iter = len(set1) * len(set2)\n",
    "    iterator = tqdm(iterator, total = n_iter)\n",
    "    \n",
    "    for (clip1_i, clip1), (clip2_i, clip2) in iterator:\n",
    "        #distance matrix\n",
    "        attr1 = torch.as_tensor(list(nx.get_node_attributes(clip1, \"attr\").values()), device=device)\n",
    "        attr2 = torch.as_tensor(list(nx.get_node_attributes(clip2, \"attr\").values()), device=device)\n",
    "        if torch.is_floating_point(attr1):\n",
    "            D = (attr1[:, None] - attr2[None, :]).square()\n",
    "            if len(D.shape)>2:\n",
    "                D = D.mean(*range(2, D.shape))\n",
    "        else:\n",
    "            D = (attr1[:, None] == attr2[None, :])\n",
    "        D = D.to(torch.float32)\n",
    "\n",
    "        #markov chains\n",
    "        markov1 = weighted_transition_matrix(clip1, q=q)\n",
    "        markov2 = weighted_transition_matrix(clip2, q=q)\n",
    "        \n",
    "        #distributions\n",
    "        mu1 = torch.ones(1, len(clip1), device=device) / len(clip1)\n",
    "        mu2 = torch.ones(1, len(clip2), device=device) / len(clip2)\n",
    "        \n",
    "        #distance\n",
    "        distance_matrix[clip1_i, clip2_i] = \\\n",
    "            discounted_wl_infty(markov1.to(device)[None, ...], \n",
    "                                markov2.to(device)[None, ...], \n",
    "                                D[None, ...], \n",
    "                                muX=mu1,\n",
    "                                muY=mu2,\n",
    "                                **wl_parameters).square()\n",
    "        \n",
    "    if set1 is set2:\n",
    "        distance_matrix = distance_matrix + distance_matrix.T\n",
    "    return distance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1147a27f-9234-4f0d-88fc-2283109e86eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function stolen from Sam\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def choose_parameters( gammas, Cs, D, y, cv=5, ksvm=False):\n",
    "    cv = StratifiedKFold(n_splits=cv)\n",
    "    results = []\n",
    "    param_pairs = []\n",
    "    for g in gammas:\n",
    "        for c in Cs:\n",
    "            param_pairs.append((g, c))\n",
    "\n",
    "    for train_index, test_index in cv.split(D, y):\n",
    "        split_results = []\n",
    "        for i in range(len(gammas)):\n",
    "            for j in range(len(Cs)):\n",
    "                g = gammas[i]\n",
    "                c = Cs[j]\n",
    "                D_train = D[train_index][:, train_index]\n",
    "                D_test = D[test_index][:, train_index]\n",
    "                K_train = np.exp(-g * D_train)\n",
    "                K_test = np.exp(-g * D_test)\n",
    "                y_train = y[train_index]\n",
    "                y_test = y[test_index]\n",
    "                clf = SVC(kernel='precomputed', C = c, max_iter=1000)\n",
    "                clf.fit(K_train, y_train)\n",
    "                y_pred = clf.predict(K_test)\n",
    "                split_results.append(accuracy_score(y_test, y_pred))\n",
    "        results.append(split_results)\n",
    "\n",
    "    results = np.array(results)\n",
    "    fin_results = results.mean(axis=0)\n",
    "    best_idx = np.argmax(fin_results)\n",
    "    return param_pairs[best_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6886bab7-4a81-4834-9ad2-0f0a822e8efc",
   "metadata": {},
   "source": [
    "## 3. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c067add9-a3e9-43e9-99e9-8ddcb7d0bbbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gammas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6e409349-a3b1-4c88-aeb6-d3ce837b3816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wl_parameters=dict(delta=.2, \n",
    "                   x_is_sparse=True, \n",
    "                   y_is_sparse=True,\n",
    "                   convergence_threshold_rtol = .01,\n",
    "                   convergence_threshold_atol = 1e-3)\n",
    "def run_experiment(data, y, distances):\n",
    "    accuracies = []\n",
    "    for train_index, test_index in tqdm(skf.split(distances, y)):\n",
    "        D_train = distances[train_index][:, train_index]\n",
    "        D_test = distances[test_index][:, train_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "        params = choose_parameters(gammas, Cs, D_train, y_train, cv=10)\n",
    "        clf = SVC(kernel='precomputed', C = params[1], max_iter=5000)\n",
    "        K_train = np.exp(-params[0]*D_train)\n",
    "        K_test = np.exp(-params[0]*D_test)\n",
    "        clf.fit(K_train, y_train)\n",
    "        y_pred = clf.predict(K_test)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b176fe4-0cc6-4fde-81ea-baf2109ac8b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:14,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MUTAG: Average accuracy: 0.6807017543859649 Standard deviation: 0.05309735421200547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/58996 [00:00<?, ?it/s]/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/ot_markov_distances/utils.py:27: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A: Tensor= torch.as_tensor(nx.adjacency_matrix(G, weight=None).todense())#type:ignore\n",
      " 97%|█████████▋| 57380/58996 [5:26:32<09:56,  2.71it/s]  IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "  8%|▊         | 4915/60726 [24:43<6:18:27,  2.46it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 18%|█▊        | 10957/60726 [1:10:39<4:37:13,  2.99it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 27%|██▋       | 16401/60726 [1:51:37<4:37:46,  2.66it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 37%|███▋      | 22650/60726 [2:25:34<3:35:37,  2.94it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 48%|████▊     | 28940/60726 [3:00:55<2:24:11,  3.67it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 53%|█████▎    | 31942/60726 [3:18:22<2:44:48,  2.91it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 58%|█████▊    | 35104/60726 [3:35:10<2:39:23,  2.68it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 63%|██████▎   | 37966/60726 [3:51:30<1:54:10,  3.32it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 68%|██████▊   | 41023/60726 [4:09:29<2:21:55,  2.31it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 72%|███████▏  | 43967/60726 [4:25:44<1:45:48,  2.64it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 77%|███████▋  | 47011/60726 [4:43:10<54:31,  4.19it/s]  IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 81%|████████  | 49142/60726 [4:55:17<45:40,  4.23it/s]  IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 86%|████████▌ | 51935/60726 [5:11:18<56:57,  2.57it/s]  IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 91%|█████████ | 55277/60726 [5:29:44<36:28,  2.49it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 96%|█████████▌| 58143/60726 [5:46:09<14:49,  2.90it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 60726/60726 [6:00:00<00:00,  2.81it/s]\n",
      "10it [00:22,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTC_FM: Average accuracy: 0.6128571428571429 Standard deviation: 0.05285714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/108811 [00:00<?, ?it/s]/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/ot_markov_distances/utils.py:27: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A: Tensor= torch.as_tensor(nx.adjacency_matrix(G, weight=None).todense())#type:ignore\n",
      "  1%|          | 697/108811 [04:46<12:36:58,  2.38it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "  3%|▎         | 3457/108811 [23:38<11:42:36,  2.50it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "  6%|▌         | 6526/108811 [44:31<11:51:24,  2.40it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "  9%|▊         | 9286/108811 [1:03:15<11:27:05,  2.41it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 11%|█▏        | 12362/108811 [1:23:58<10:43:01,  2.50it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 14%|█▍        | 15100/108811 [1:42:31<10:04:01,  2.59it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 16%|█▌        | 17168/108811 [1:56:09<10:31:04,  2.42it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 19%|█▉        | 20982/108811 [2:21:09<9:22:54,  2.60it/s] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 21%|██        | 23009/108811 [2:34:20<9:08:58,  2.60it/s] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 25%|██▍       | 26804/108811 [2:59:29<9:25:12,  2.42it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 27%|██▋       | 28972/108811 [3:13:36<8:34:55,  2.58it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 30%|███       | 32650/108811 [3:37:41<8:17:37,  2.55it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 32%|███▏      | 34957/108811 [3:53:02<7:54:39,  2.59it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 35%|███▌      | 38468/108811 [4:16:15<7:38:49,  2.56it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 91%|█████████▏| 99356/108811 [12:33:10<1:09:36,  2.26it/s]"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for (data, y), name, i in zip(datasets, dataset_titles, it.count()):\n",
    "    if distance_matrices[i] is None:\n",
    "        distance_matrices[i] = compute_distance_matrix(data, data, wl_parameters).numpy(force=True)\n",
    "    distance_matrix = distance_matrices[i]\n",
    "    accuracies = run_experiment(data, y, distance_matrix)\n",
    "    #print(\"Done with WWL experiments\")\n",
    "    print(f\"{name}: Average accuracy:\", np.mean(accuracies), \"Standard deviation:\", np.std(accuracies))\n",
    "    results.append(accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2c65e873-83af-4886-b86a-8ca65574d386",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of ot_markov_distances.sinkhorn failed: Traceback (most recent call last):\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 261, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 484, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 381, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 333, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 381, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 333, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 381, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 333, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 381, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 333, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "RecursionError: maximum recursion depth exceeded\n",
      "]\n",
      "[autoreload of ot_markov_distances.discounted_wl failed: Traceback (most recent call last):\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 261, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 484, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 381, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 333, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 381, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 333, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 381, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 333, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 381, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/tristan/miniconda3/envs/default/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 333, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "RecursionError: maximum recursion depth exceeded\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.631578947368421,\n",
       "  0.7894736842105263,\n",
       "  0.631578947368421,\n",
       "  0.6842105263157895,\n",
       "  0.7368421052631579,\n",
       "  0.631578947368421,\n",
       "  0.7368421052631579,\n",
       "  0.631578947368421,\n",
       "  0.6666666666666666,\n",
       "  0.6666666666666666],\n",
       " [0.5428571428571428,\n",
       "  0.5428571428571428,\n",
       "  0.5714285714285714,\n",
       "  0.5714285714285714,\n",
       "  0.5588235294117647,\n",
       "  0.5588235294117647,\n",
       "  0.5588235294117647,\n",
       "  0.5588235294117647,\n",
       "  0.5588235294117647,\n",
       "  0.5588235294117647],\n",
       " [0.6571428571428571,\n",
       "  0.6,\n",
       "  0.5714285714285714,\n",
       "  0.6,\n",
       "  0.6,\n",
       "  0.6285714285714286,\n",
       "  0.6,\n",
       "  0.6857142857142857,\n",
       "  0.6857142857142857,\n",
       "  0.5],\n",
       " [0.7872340425531915,\n",
       "  0.7872340425531915,\n",
       "  0.7872340425531915,\n",
       "  0.7872340425531915,\n",
       "  0.7872340425531915,\n",
       "  0.7659574468085106,\n",
       "  0.7659574468085106,\n",
       "  0.782608695652174,\n",
       "  0.782608695652174,\n",
       "  0.782608695652174],\n",
       " [0.6160714285714286,\n",
       "  0.7678571428571429,\n",
       "  0.6696428571428571,\n",
       "  0.6216216216216216,\n",
       "  0.5855855855855856,\n",
       "  0.6576576576576577,\n",
       "  0.7027027027027027,\n",
       "  0.6666666666666666,\n",
       "  0.6036036036036037,\n",
       "  0.5945945945945946]]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e9e29dd-cd94-4a51-b5ab-bf50242bf58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "WL = grakel.WeisfeilerLehman()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3edfb2d-f76e-428c-91eb-71c72563ed6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported input type. For more information check the documentation, concerning valid input types for graph type object.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m WL\u001b[38;5;241m.\u001b[39mfit_transform(ds)\n",
      "File \u001b[0;32m~/miniconda3/envs/default/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/default/lib/python3.11/site-packages/grakel/kernels/weisfeiler_lehman.py:298\u001b[0m, in \u001b[0;36mWeisfeilerLehman.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform input cannot be None\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     km, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_input(X)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_X_diag \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiagonal(km)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize:\n",
      "File \u001b[0;32m~/miniconda3/envs/default/lib/python3.11/site-packages/grakel/kernels/weisfeiler_lehman.py:158\u001b[0m, in \u001b[0;36mWeisfeilerLehman.parse_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    157\u001b[0m         extra \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(x[\u001b[38;5;241m3\u001b[39m:])\n\u001b[0;32m--> 158\u001b[0m     x \u001b[38;5;241m=\u001b[39m Graph(x[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m1\u001b[39m], x[\u001b[38;5;241m2\u001b[39m], graph_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_format)\n\u001b[1;32m    159\u001b[0m     extra \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mget_labels(purpose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_format,\n\u001b[1;32m    160\u001b[0m                           label_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), ) \u001b[38;5;241m+\u001b[39m extra\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/default/lib/python3.11/site-packages/grakel/graph.py:161\u001b[0m, in \u001b[0;36mGraph.__init__\u001b[0;34m(self, initialization_object, node_labels, edge_labels, graph_format, construct_labels)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format \u001b[38;5;241m=\u001b[39m graph_format\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (initialization_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_graph(initialization_object,\n\u001b[1;32m    162\u001b[0m                      node_labels,\n\u001b[1;32m    163\u001b[0m                      edge_labels)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m graph_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno initialization object \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    166\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m- format must not be auto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/default/lib/python3.11/site-packages/grakel/graph.py:218\u001b[0m, in \u001b[0;36mGraph.build_graph\u001b[0;34m(self, g, node_labels, edge_labels)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdictionary\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    219\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported input type. For more information \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    220\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheck the documentation, concerning valid input \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtypes for graph type object.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# If graph is of one type prune the other\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjacency\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported input type. For more information check the documentation, concerning valid input types for graph type object."
     ]
    }
   ],
   "source": [
    "WL.fit_transform(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b1e5fc2-35db-43c9-9c31-1da9d4d0162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(f\"{os.environ['HOME']}/software/tudataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb01428f-5a28-4b76-8833-b2a2ed50b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tud_benchmark import kernel_baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7e973d59-61ce-4732-bcf2-270431eb6426",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = \"ENZYMES\"\n",
    "classes = dp.get_dataset(test_ds)\n",
    "gram_matrix = kernel_baselines.compute_wl_1_dense(test_ds, 3, True, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1c6e4c3a-64a9-4547-8f45-09030eb2990d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 964.,  502.,  585., ...,  973.,  998.,  983.],\n",
       "       [ 502.,  408.,  362., ...,  591.,  615.,  610.],\n",
       "       [ 585.,  362.,  546., ...,  587.,  678.,  674.],\n",
       "       ...,\n",
       "       [ 973.,  591.,  587., ..., 2054., 1417., 1387.],\n",
       "       [ 998.,  615.,  678., ..., 1417., 1596., 1501.],\n",
       "       [ 983.,  610.,  674., ..., 1387., 1501., 1652.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ac9da7b9-eb4c-4b4f-85be-a25116c56f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8524c550-1ce7-4e0d-b570-6a09c35dc587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tristan/software/tudataset/tud_benchmark\n"
     ]
    }
   ],
   "source": [
    "cd /home/tristan/software/tudataset/tud_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be3af33-f848-4fa7-956c-e72764efa56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
